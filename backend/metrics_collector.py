"""
Sistema completo de coleta de m√©tricas em tempo real para agentes IA
Usando Redis para filas ass√≠ncronas e processamento inteligente
"""

import asyncio
import json
import time
import re
import uuid
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from sqlalchemy.orm import Session
from sqlalchemy import text
import redis.asyncio as redis
import logging
from database import get_db
import agent_models

logger = logging.getLogger(__name__)

class MetricsCollector:
    """Coletor principal de m√©tricas com Redis"""
    
    def __init__(self, redis_url: str = "redis://redis:6379"):
        self.redis_url = redis_url
        self.redis_client = None
        self.running = False
        
        # Filas Redis
        self.EXECUTION_QUEUE = "metrics:execution"
        self.CONTENT_QUEUE = "metrics:content" 
        self.SESSION_QUEUE = "metrics:session"
        self.CLASSIFICATION_QUEUE = "metrics:classification"
        
        # Cache para sess√µes ativas
        self.active_sessions = {}
        
    async def initialize(self):
        """Inicializar conex√£o Redis"""
        try:
            self.redis_client = redis.from_url(self.redis_url, decode_responses=True)
            await self.redis_client.ping()
            logger.info("üîó Redis conectado para m√©tricas")
            return True
        except Exception as e:
            logger.error(f"‚ùå Erro ao conectar Redis: {e}")
            return False
    
    async def start_workers(self):
        """Iniciar workers ass√≠ncronos"""
        if not self.redis_client:
            await self.initialize()
        
        self.running = True
        
        # Iniciar workers em paralelo
        workers = [
            self._execution_worker(),
            self._content_worker(), 
            self._session_worker(),
            self._classification_worker(),
            self._cleanup_worker(),
            self._auto_analysis_worker()  # Novo worker para an√°lise autom√°tica
        ]
        
        logger.info("üöÄ Workers de m√©tricas iniciados")
        await asyncio.gather(*workers)
    
    async def stop_workers(self):
        """Parar workers"""
        self.running = False
        if self.redis_client:
            await self.redis_client.close()
        logger.info("‚èπÔ∏è Workers de m√©tricas parados")
    
    # ===============================
    # COLETA DE M√âTRICAS
    # ===============================
    
    async def collect_execution_metrics(self, data: Dict[str, Any]):
        """Coletar m√©tricas de execu√ß√£o de agente"""
        try:
            # Verificar se Redis est√° conectado
            if not self.redis_client:
                logger.warning("‚ö†Ô∏è Redis n√£o conectado - salvando m√©trica diretamente no banco")
                await self._save_execution_to_db(data)
                return
            
            # Se n√£o h√° tokens reais, estimar
            if not data.get('input_tokens') and data.get('input_text'):
                data['input_tokens'] = self.estimate_tokens(data['input_text'])
            if not data.get('output_tokens') and data.get('output_text'):
                data['output_tokens'] = self.estimate_tokens(data['output_text'])
            
            # Calcular custo se n√£o fornecido
            if not data.get('cost_estimate') and data.get('input_tokens') and data.get('output_tokens'):
                data['cost_estimate'] = self.calculate_cost(
                    data.get('model', 'gpt-4o-mini'),
                    data['input_tokens'],
                    data['output_tokens']
                )
            
            await self.redis_client.lpush(self.EXECUTION_QUEUE, json.dumps(data))
            logger.debug(f"üìä M√©trica de execu√ß√£o coletada: {data.get('agent_id', 'N/A')}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao coletar m√©trica de execu√ß√£o: {e}")
    
    async def collect_content_metrics(self, data: Dict[str, Any]):
        """Coletar m√©tricas de conte√∫do para an√°lise de t√≥picos"""
        try:
            if not self.redis_client:
                logger.warning("‚ö†Ô∏è Redis n√£o conectado - salvando conte√∫do diretamente no banco")
                await self._save_content_to_db(data)
                return
                
            await self.redis_client.lpush(self.CONTENT_QUEUE, json.dumps(data))
            logger.debug(f"üìù M√©trica de conte√∫do coletada: {data.get('session_id', 'N/A')}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao coletar m√©trica de conte√∫do: {e}")
    
    async def collect_session_metrics(self, data: Dict[str, Any]):
        """Coletar m√©tricas de sess√£o de chat"""
        try:
            if not self.redis_client:
                logger.warning("‚ö†Ô∏è Redis n√£o conectado - salvando sess√£o diretamente no banco")
                await self._save_session_to_db(data)
                return
                
            await self.redis_client.lpush(self.SESSION_QUEUE, json.dumps(data))
            
            # Atualizar cache de sess√µes ativas
            session_id = data.get('session_id')
            if session_id:
                self.active_sessions[session_id] = {
                    'user_id': data.get('user_id'),
                    'agent_id': data.get('agent_id'),
                    'team_id': data.get('team_id'),
                    'start_time': data.get('timestamp', datetime.now().isoformat()),
                    'message_count': data.get('message_count', 1),
                    'last_activity': datetime.now().isoformat()
                }
            
            logger.debug(f"üí¨ M√©trica de sess√£o coletada: {session_id}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao coletar m√©trica de sess√£o: {e}")
    
    async def request_conversation_classification(self, session_id: str, conversation_data: Dict[str, Any]):
        """Solicitar classifica√ß√£o inteligente de conversa"""
        try:
            classification_data = {
                'session_id': session_id,
                'conversation_data': conversation_data,
                'timestamp': datetime.now().isoformat(),
                'request_id': str(uuid.uuid4())
            }
            
            # Verificar se Redis est√° dispon√≠vel
            if not self.redis_client:
                logger.warning("‚ö†Ô∏è Redis n√£o conectado - processando classifica√ß√£o diretamente")
                await self._process_conversation_classification(classification_data)
                return
            
            await self.redis_client.lpush(self.CLASSIFICATION_QUEUE, json.dumps(classification_data))
            logger.info(f"ü§ñ Classifica√ß√£o solicitada para sess√£o: {session_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao solicitar classifica√ß√£o: {e}")
            # Fallback direto para processamento
            try:
                classification_data = {
                    'session_id': session_id,
                    'conversation_data': conversation_data,
                    'timestamp': datetime.now().isoformat(),
                    'request_id': str(uuid.uuid4())
                }
                await self._process_conversation_classification(classification_data)
            except Exception as fallback_error:
                logger.error(f"‚ùå Erro no fallback de classifica√ß√£o: {fallback_error}")
    
    # ===============================
    # WORKERS ASS√çNCRONOS
    # ===============================
    
    async def _execution_worker(self):
        """Worker para processar m√©tricas de execu√ß√£o"""
        logger.info("üîÑ Worker de execu√ß√£o iniciado")
        
        while self.running:
            try:
                # Processar em lote para performance
                items = []
                for _ in range(10):  # Lote de at√© 10 itens
                    item = await self.redis_client.brpop(self.EXECUTION_QUEUE, timeout=1)
                    if item:
                        items.append(json.loads(item[1]))
                    else:
                        break
                
                if items:
                    await self._process_execution_batch(items)
                    
            except Exception as e:
                logger.error(f"‚ùå Erro no worker de execu√ß√£o: {e}")
                await asyncio.sleep(1)
    
    async def _content_worker(self):
        """Worker para processar an√°lise de conte√∫do"""
        logger.info("üîÑ Worker de conte√∫do iniciado")
        
        while self.running:
            try:
                item = await self.redis_client.brpop(self.CONTENT_QUEUE, timeout=5)
                if item:
                    data = json.loads(item[1])
                    await self._process_content_analysis(data)
                    
            except Exception as e:
                logger.error(f"‚ùå Erro no worker de conte√∫do: {e}")
                await asyncio.sleep(1)
    
    async def _session_worker(self):
        """Worker para processar m√©tricas de sess√£o"""
        logger.info("üîÑ Worker de sess√£o iniciado")
        
        while self.running:
            try:
                item = await self.redis_client.brpop(self.SESSION_QUEUE, timeout=5)
                if item:
                    data = json.loads(item[1])
                    await self._process_session_metrics(data)
                    
            except Exception as e:
                logger.error(f"‚ùå Erro no worker de sess√£o: {e}")
                await asyncio.sleep(1)
    
    async def _classification_worker(self):
        """Worker para classifica√ß√£o inteligente de conversas"""
        logger.info("üîÑ Worker de classifica√ß√£o iniciado")
        
        while self.running:
            try:
                item = await self.redis_client.brpop(self.CLASSIFICATION_QUEUE, timeout=10)
                if item:
                    data = json.loads(item[1])
                    await self._process_conversation_classification(data)
                    
            except Exception as e:
                logger.error(f"‚ùå Erro no worker de classifica√ß√£o: {e}")
                await asyncio.sleep(1)
    
    async def _cleanup_worker(self):
        """Worker para limpeza de dados antigos"""
        logger.info("üîÑ Worker de limpeza iniciado")
        
        while self.running:
            try:
                # Executar limpeza a cada 1 hora
                await asyncio.sleep(3600)
                await self._cleanup_old_data()
                
            except Exception as e:
                logger.error(f"‚ùå Erro no worker de limpeza: {e}")

    async def _auto_analysis_worker(self):
        """Worker para an√°lise autom√°tica por timeout"""
        logger.info("üîÑ Worker de an√°lise autom√°tica iniciado")
        
        while self.running:
            try:
                # Verificar sess√µes inativas a cada 5 minutos
                await asyncio.sleep(300)
                await self._check_inactive_sessions()
                
            except Exception as e:
                logger.error(f"‚ùå Erro no worker de an√°lise autom√°tica: {e}")
    
    # ===============================
    # PROCESSADORES
    # ===============================
    
    async def _process_execution_batch(self, items: List[Dict[str, Any]]):
        """Processar lote de m√©tricas de execu√ß√£o"""
        from database import SessionLocal
        
        db = SessionLocal()
        try:
            for item in items:
                # Salvar m√©tricas de tokens
                if item.get('input_tokens') or item.get('output_tokens'):
                    db.execute(text("""
                        INSERT INTO token_usage (
                            agent_id, session_id, model_used, input_tokens, 
                            output_tokens, cost_estimate, operation_type, created_at
                        ) VALUES (
                            :agent_id, :session_id, :model, :input_tokens,
                            :output_tokens, :cost, :operation_type, :timestamp
                        )
                    """), {
                        'agent_id': item.get('agent_id'),
                        'session_id': item.get('session_id'),
                        'model': item.get('model', 'unknown'),
                        'input_tokens': item.get('input_tokens', 0),
                        'output_tokens': item.get('output_tokens', 0),
                        'cost': item.get('cost_estimate', 0.0),
                        'operation_type': item.get('operation_type', 'chat'),
                        'timestamp': item.get('timestamp', datetime.now())
                    })
                
                # NOVO: Salvar na tabela agent_executions para c√°lculo de tempo m√©dio
                if item.get('agent_id'):
                    db.execute(text("""
                        INSERT INTO agent_executions (
                            agent_id, input_text, output_text, tools_used, 
                            execution_time_ms, tokens_used, created_at
                        ) VALUES (
                            :agent_id, :input_text, :output_text, :tools_used,
                            :execution_time_ms, :tokens_used, :timestamp
                        )
                    """), {
                        'agent_id': item.get('agent_id'),
                        'input_text': str(item.get('input_text', ''))[:1000],
                        'output_text': str(item.get('output_text', ''))[:2000],
                        'tools_used': str(item.get('tools_used', [])),
                        'execution_time_ms': item.get('execution_time', item.get('execution_time_ms', 0)),
                        'tokens_used': item.get('input_tokens', 0) + item.get('output_tokens', 0),
                        'timestamp': item.get('timestamp', datetime.now())
                    })
                
                # Atualizar performance_metrics
                db.execute(text("""
                    INSERT INTO performance_metrics (
                        agent_id, metric_date, total_interactions, 
                        avg_response_time_ms, tokens_consumed
                    ) VALUES (
                        :agent_id, :date, 1, :response_time, :tokens
                    ) ON CONFLICT (agent_id, metric_date) 
                    DO UPDATE SET
                        total_interactions = performance_metrics.total_interactions + 1,
                        avg_response_time_ms = (
                            performance_metrics.avg_response_time_ms + :response_time
                        ) / 2,
                        tokens_consumed = performance_metrics.tokens_consumed + :tokens
                """), {
                    'agent_id': item.get('agent_id'),
                    'date': datetime.now().date(),
                    'response_time': item.get('execution_time_ms', 0),
                    'tokens': (item.get('input_tokens', 0) + item.get('output_tokens', 0))
                })
            
            db.commit()
            logger.debug(f"üíæ Processadas {len(items)} m√©tricas de execu√ß√£o")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar lote de execu√ß√£o: {e}")
            db.rollback()
        finally:
            db.close()
    
    async def _process_content_analysis(self, data: Dict[str, Any]):
        """Processar an√°lise de conte√∫do para extrair t√≥picos"""
        from database import SessionLocal
        
        db = SessionLocal()
        try:
            content = data.get('message_content', '')
            
            # Extra√ß√£o simples de t√≥picos usando regex
            topics = self._extract_topics(content)
            keywords = self._extract_keywords(content)
            
            if topics:
                import json as json_lib
                db.execute(text("""
                    INSERT INTO content_topics (
                        session_id, agent_id, extracted_topics, 
                        message_content, topic_keywords, confidence_score, created_at
                    ) VALUES (
                        :session_id, :agent_id, :topics, 
                        :content, :keywords, :confidence, :timestamp
                    )
                """), {
                    'session_id': data.get('session_id'),
                    'agent_id': data.get('agent_id'),
                    'topics': json_lib.dumps(topics),
                    'content': content[:500],  # Limitar tamanho
                    'keywords': json_lib.dumps(keywords),
                    'confidence': data.get('confidence_score', 0.8),
                    'timestamp': data.get('timestamp', datetime.now())
                })
                
                db.commit()
                logger.debug(f"üè∑Ô∏è T√≥picos extra√≠dos: {topics}")
        
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar an√°lise de conte√∫do: {e}")
            db.rollback()
        finally:
            db.close()
    
    async def _process_session_metrics(self, data: Dict[str, Any]):
        """Processar m√©tricas de sess√£o"""
        from database import SessionLocal
        
        db = SessionLocal()
        try:
            session_id = data.get('session_id')
            
            # Inserir ou atualizar user_metrics
            db.execute(text("""
                INSERT INTO user_metrics (
                    user_id, session_id, agent_id, total_messages,
                    session_duration_seconds, created_at, updated_at
                ) VALUES (
                    :user_id, :session_id, :agent_id, :messages,
                    :duration, :timestamp, :timestamp
                ) ON CONFLICT (user_id, session_id)
                DO UPDATE SET
                    total_messages = user_metrics.total_messages + :messages,
                    updated_at = :timestamp
            """), {
                'user_id': data.get('user_id', 'anonymous'),
                'session_id': session_id,
                'agent_id': data.get('agent_id'),
                'messages': data.get('message_count', 1),
                'duration': data.get('duration_seconds', 0),
                'timestamp': data.get('timestamp', datetime.now())
            })
            
            db.commit()
            logger.debug(f"üë§ M√©tricas de sess√£o processadas: {session_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar m√©tricas de sess√£o: {e}")
            db.rollback()
        finally:
            db.close()
    
    async def _process_conversation_classification(self, data: Dict[str, Any]):
        """Processar classifica√ß√£o inteligente usando agente especialista"""
        try:
            from agents import AgentManager
            from qdrant_service import QdrantService
            from database import SessionLocal
            
            db = SessionLocal()
            qdrant_service = QdrantService()
            agent_manager = AgentManager(db, qdrant_service)
            
            # Configura√ß√£o do agente classificador
            classifier_config = {
                "id": 9999,  # ID especial para classificador
                "name": "Classificador de Conversas",
                "role": "Especialista em an√°lise e classifica√ß√£o de conversas",
                "model": "gpt-4o-mini",
                "temperature": 0.1,
                "instructions": """
                Voc√™ √© um especialista em an√°lise de conversas de suporte t√©cnico.
                
                Analise a conversa fornecida e extraia:
                1. T√≥picos principais mencionados
                2. Sentimento do usu√°rio (positivo/negativo/neutro)
                3. N√≠vel de satisfa√ß√£o estimado (1-5)
                4. Categoria do problema (t√©cnico, comercial, suporte, etc.)
                5. Complexidade da solu√ß√£o (baixa, m√©dia, alta)
                6. Palavras-chave relevantes
                
                Responda APENAS em formato JSON:
                {
                    "topics": ["t√≥pico1", "t√≥pico2"],
                    "sentiment": "positivo|negativo|neutro",
                    "satisfaction": 1-5,
                    "category": "categoria",
                    "complexity": "baixa|m√©dia|alta",
                    "keywords": ["palavra1", "palavra2"],
                    "summary": "resumo de 1 linha"
                }
                """,
                "tools_config": []
            }
            
            # Preparar prompt com a conversa
            conversation_data = data.get('conversation_data', {})
            messages = conversation_data.get('messages', [])
            
            conversation_text = "\n".join([
                f"{msg.get('sender', 'user')}: {msg.get('content', '')[:200]}"
                for msg in messages[-10:]  # √öltimas 10 mensagens
            ])
            
            prompt = f"""
            Analise esta conversa de suporte t√©cnico:
            
            {conversation_text}
            
            Forne√ßa a an√°lise em formato JSON conforme instru√≠do.
            """
            
            # Executar classifica√ß√£o
            result = agent_manager.execute_agent_task(
                classifier_config, 
                prompt, 
                f"classification_{data.get('session_id')}"
            )
            
            if result.get('success'):
                try:
                    # Tentar parsear JSON da resposta
                    response_text = result.get('response', '{}')
                    # Extrair JSON se estiver dentro de c√≥digo markdown
                    if '```json' in response_text:
                        response_text = response_text.split('```json')[1].split('```')[0]
                    elif '```' in response_text:
                        response_text = response_text.split('```')[1].split('```')[0]
                    
                    classification = json.loads(response_text)
                    
                    # Salvar classifica√ß√£o no banco
                    db.execute(text("""
                        INSERT INTO user_feedback (
                            session_id, user_id, agent_id, rating, 
                            issue_category, feedback_comment, created_at
                        ) VALUES (
                            :session_id, :user_id, :agent_id, :rating,
                            :category, :summary, :timestamp
                        )
                    """), {
                        'session_id': data.get('session_id'),
                        'user_id': conversation_data.get('user_id', 'classified'),
                        'agent_id': conversation_data.get('agent_id'),
                        'rating': classification.get('satisfaction', 3),
                        'category': classification.get('category', 'geral'),
                        'summary': classification.get('summary', 'Classifica√ß√£o autom√°tica'),
                        'timestamp': datetime.now()
                    })
                    
                    # Salvar t√≥picos extra√≠dos
                    if classification.get('topics'):
                        import json as json_lib
                        db.execute(text("""
                            INSERT INTO content_topics (
                                session_id, agent_id, extracted_topics,
                                topic_keywords, confidence_score, created_at
                            ) VALUES (
                                :session_id, :agent_id, :topics,
                                :keywords, :confidence, :timestamp
                            )
                        """), {
                            'session_id': data.get('session_id'),
                            'agent_id': conversation_data.get('agent_id'),
                            'topics': json_lib.dumps(classification.get('topics', [])),
                            'keywords': json_lib.dumps(classification.get('keywords', [])),
                            'confidence': 0.9,  # Alta confian√ßa para classifica√ß√£o IA
                            'timestamp': datetime.now()
                        })
                    
                    db.commit()
                    logger.info(f"üéØ Conversa classificada: {classification.get('summary', 'N/A')}")
                
                except json.JSONDecodeError as e:
                    logger.warning(f"‚ö†Ô∏è Resposta de classifica√ß√£o n√£o √© JSON v√°lido: {e}")
                    
            else:
                logger.warning(f"‚ö†Ô∏è Falha na classifica√ß√£o da conversa: {result.get('error', 'N/A')}")
                
            db.close()
            
        except Exception as e:
            logger.error(f"‚ùå Erro na classifica√ß√£o de conversa: {e}")
    
    async def _cleanup_old_data(self):
        """Limpar dados antigos para otimizar performance"""
        from database import SessionLocal
        
        db = SessionLocal()
        try:
            # Limpar sess√µes ativas antigas (mais de 24h)
            cutoff_time = datetime.now() - timedelta(hours=24)
            old_sessions = [
                sid for sid, data in self.active_sessions.items()
                if datetime.fromisoformat(data['last_activity']) < cutoff_time
            ]
            
            for session_id in old_sessions:
                del self.active_sessions[session_id]
            
            logger.info(f"üßπ Limpeza conclu√≠da: {len(old_sessions)} sess√µes removidas")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na limpeza: {e}")
        finally:
            db.close()
    
    async def _check_inactive_sessions(self):
        """Verificar sess√µes inativas para an√°lise autom√°tica por timeout"""
        from database import SessionLocal
        from chat_service import ChatService
        
        db = SessionLocal()
        chat_service = ChatService(db)
        
        try:
            # Buscar sess√µes com √∫ltima atividade h√° mais de 15 minutos
            cutoff_time = datetime.now() - timedelta(minutes=15)
            
            inactive_sessions = db.execute(text("""
                SELECT DISTINCT cs.session_id, cs.team_id, cs.last_activity
                FROM chat_sessions cs
                WHERE cs.last_activity < :cutoff_time
                AND cs.session_id NOT IN (
                    SELECT session_id FROM user_feedback 
                    WHERE session_id = cs.session_id
                    AND auto_generated = true
                )
                ORDER BY cs.last_activity DESC
                LIMIT 10
            """), {'cutoff_time': cutoff_time}).fetchall()
            
            analyzed_count = 0
            
            for session_record in inactive_sessions:
                session_id = session_record.session_id
                team_id = session_record.team_id
                
                try:
                    # Obter mensagens da sess√£o
                    messages = chat_service.get_chat_history(session_id)
                    
                    if messages and len(messages) >= 3:  # M√≠nimo 3 mensagens
                        conversation_data = {
                            'session_id': session_id,
                            'messages': messages,
                            'total_messages': len(messages),
                            'auto_triggered': True,
                            'trigger_reason': 'inactivity_timeout',
                            'user_id': 'auto_timeout',
                            'team_id': team_id,
                            'inactive_since': session_record.last_activity.isoformat()
                        }
                        
                        # Solicitar classifica√ß√£o
                        await self.request_conversation_classification(session_id, conversation_data)
                        analyzed_count += 1
                        
                        logger.info(f"‚è∞ An√°lise por timeout disparada: {session_id} (inativo desde {session_record.last_activity})")
                
                except Exception as e:
                    logger.error(f"‚ùå Erro ao analisar sess√£o inativa {session_id}: {e}")
            
            if analyzed_count > 0:
                logger.info(f"‚è∞ Total de an√°lises por timeout disparadas: {analyzed_count}")
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao verificar sess√µes inativas: {e}")
        finally:
            db.close()
    
    # ===============================
    # UTILIT√ÅRIOS
    # ===============================
    
    def _extract_topics(self, text: str) -> List[str]:
        """Extrair t√≥picos usando regex simples"""
        # Lista de t√≥picos t√©cnicos comuns
        topic_patterns = {
            'freios': r'\b(?:freio|freios|frenagem|pastilha|disco|tambor)\b',
            'motor': r'\b(?:motor|motores|arranque|partida|combust√£o)\b',
            'hidr√°ulico': r'\b(?:hidr√°ulico|hidr√°ulica|√≥leo|fluido|press√£o)\b',
            'el√©trico': r'\b(?:el√©trico|el√©trica|bateria|alternador|fia√ß√£o)\b',
            'pneu': r'\b(?:pneu|pneus|roda|rodas|press√£o|calibragem)\b',
            'manuten√ß√£o': r'\b(?:manuten√ß√£o|manuten√ß√µes|preventiva|corretiva|revis√£o)\b',
            'pe√ßas': r'\b(?:pe√ßa|pe√ßas|componente|componentes|reposi√ß√£o)\b',
            'problema': r'\b(?:problema|problemas|defeito|defeitos|falha|falhas)\b',
            'configura√ß√£o': r'\b(?:configura√ß√£o|configurar|ajuste|calibra√ß√£o)\b'
        }
        
        topics = []
        text_lower = text.lower()
        
        for topic, pattern in topic_patterns.items():
            if re.search(pattern, text_lower):
                topics.append(topic)
        
        return topics
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extrair palavras-chave relevantes"""
        # Remover stop words e extrair palavras significativas
        stop_words = {'a', 'o', 'e', 'de', 'da', 'do', 'em', 'um', 'uma', 'para', 'com', 'n√£o', 'que', 'se', 'por'}
        
        words = re.findall(r'\b\w{3,}\b', text.lower())
        keywords = [word for word in words if word not in stop_words]
        
        # Retornar top 10 palavras mais frequentes
        from collections import Counter
        return [word for word, count in Counter(keywords).most_common(10)]
    
    def estimate_tokens(self, text: str) -> int:
        """Estimar tokens baseado no texto (m√©todo simples)"""
        if not text:
            return 0
        
        # Estimativa: ~4 caracteres = 1 token
        return len(text) // 4
    
    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """Calcular custo estimado baseado no modelo"""
        # Pre√ßos aproximados por 1M tokens (2025)
        pricing = {
            'gpt-5': {'input': 15.0, 'output': 60.0},
            'gpt-5-mini': {'input': 2.0, 'output': 8.0},
            'gpt-4.1': {'input': 10.0, 'output': 30.0},
            'gpt-4o': {'input': 5.0, 'output': 15.0},
            'gpt-4o-mini': {'input': 0.15, 'output': 0.60},
            'claude-opus-4.1': {'input': 15.0, 'output': 75.0},
            'claude-sonnet-4': {'input': 3.0, 'output': 15.0},
            'claude-sonnet-3.7': {'input': 3.0, 'output': 15.0},
            'claude-3.5-sonnet': {'input': 3.0, 'output': 15.0}
        }
        
        model_pricing = pricing.get(model, {'input': 1.0, 'output': 3.0})
        
        input_cost = (input_tokens / 1_000_000) * model_pricing['input']
        output_cost = (output_tokens / 1_000_000) * model_pricing['output']
        
        return input_cost + output_cost
    
    # ===============================
    # APIs P√öBLICAS
    # ===============================
    
    async def get_active_sessions_count(self) -> int:
        """Obter n√∫mero de sess√µes ativas"""
        return len(self.active_sessions)
    
    async def get_real_time_metrics(self) -> Dict[str, Any]:
        """Obter m√©tricas em tempo real"""
        try:
            return {
                'active_sessions': len(self.active_sessions),
                'active_users': len(set(s['user_id'] for s in self.active_sessions.values())),
                'queue_sizes': {
                    'execution': await self.redis_client.llen(self.EXECUTION_QUEUE),
                    'content': await self.redis_client.llen(self.CONTENT_QUEUE),
                    'session': await self.redis_client.llen(self.SESSION_QUEUE),
                    'classification': await self.redis_client.llen(self.CLASSIFICATION_QUEUE)
                },
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erro ao obter m√©tricas em tempo real: {e}")
            return {}
    
    # ===============================
    # FALLBACK PARA SALVAMENTO DIRETO (SEM REDIS)
    # ===============================
    
    async def _save_execution_to_db(self, data: Dict[str, Any]):
        """Salvar m√©tricas de execu√ß√£o diretamente no banco (fallback)"""
        from database import SessionLocal
        
        db = SessionLocal()
        try:
            # Processar dados como no worker
            if not data.get('input_tokens') and data.get('input_text'):
                data['input_tokens'] = self.estimate_tokens(data['input_text'])
            if not data.get('output_tokens') and data.get('output_text'):
                data['output_tokens'] = self.estimate_tokens(data['output_text'])
            if not data.get('cost_estimate'):
                data['cost_estimate'] = self.calculate_cost(
                    data.get('model', 'gpt-4o-mini'),
                    data.get('input_tokens', 0),
                    data.get('output_tokens', 0)
                )
            
            # Salvar token usage
            db.execute(text("""
                INSERT INTO token_usage (
                    agent_id, session_id, model_used, input_tokens, 
                    output_tokens, cost_estimate, operation_type, created_at
                ) VALUES (
                    :agent_id, :session_id, :model, :input_tokens,
                    :output_tokens, :cost, :operation_type, :timestamp
                )
            """), {
                'agent_id': data.get('agent_id'),
                'session_id': data.get('session_id'),
                'model': data.get('model', 'gpt-4o-mini'),
                'input_tokens': data.get('input_tokens', 0),
                'output_tokens': data.get('output_tokens', 0),
                'cost': data.get('cost_estimate', 0.0),
                'operation_type': data.get('operation_type', 'chat'),
                'timestamp': datetime.now()
            })
            
            # NOVO: Salvar na tabela agent_executions para c√°lculo de tempo m√©dio
            db.execute(text("""
                INSERT INTO agent_executions (
                    agent_id, input_text, output_text, tools_used, 
                    execution_time_ms, tokens_used, created_at
                ) VALUES (
                    :agent_id, :input_text, :output_text, :tools_used,
                    :execution_time_ms, :tokens_used, :timestamp
                )
            """), {
                'agent_id': data.get('agent_id'),
                'input_text': data.get('input_text', '')[:1000],  # Limitar tamanho
                'output_text': data.get('output_text', '')[:2000],  # Limitar tamanho
                'tools_used': str(data.get('tools_used', [])),
                'execution_time_ms': data.get('execution_time', data.get('execution_time_ms', 0)),
                'tokens_used': data.get('input_tokens', 0) + data.get('output_tokens', 0),
                'timestamp': datetime.now()
            })
            
            db.commit()
            logger.debug(f"üíæ M√©trica de execu√ß√£o salva diretamente no banco")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar m√©trica de execu√ß√£o no banco: {e}")
            db.rollback()
        finally:
            db.close()
    
    async def _save_content_to_db(self, data: Dict[str, Any]):
        """Salvar m√©tricas de conte√∫do diretamente no banco (fallback)"""
        from database import SessionLocal
        
        db = SessionLocal()
        try:
            content = data.get('message_content', '')
            topics = self._extract_topics(content)
            keywords = self._extract_keywords(content)
            
            if topics:
                import json as json_lib
                db.execute(text("""
                    INSERT INTO content_topics (
                        session_id, agent_id, extracted_topics, 
                        message_content, topic_keywords, confidence_score, created_at
                    ) VALUES (
                        :session_id, :agent_id, :topics, 
                        :content, :keywords, :confidence, :timestamp
                    )
                """), {
                    'session_id': data.get('session_id'),
                    'agent_id': data.get('agent_id'),
                    'topics': json_lib.dumps(topics),
                    'content': content[:500],
                    'keywords': json_lib.dumps(keywords),
                    'confidence': 0.8,
                    'timestamp': datetime.now()
                })
                
                db.commit()
                logger.debug(f"üè∑Ô∏è T√≥picos salvos diretamente no banco: {topics}")
        
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar conte√∫do no banco: {e}")
            db.rollback()
        finally:
            db.close()
    
    async def _save_session_to_db(self, data: Dict[str, Any]):
        """Salvar m√©tricas de sess√£o diretamente no banco (fallback)"""
        from database import SessionLocal
        
        db = SessionLocal()
        try:
            db.execute(text("""
                INSERT INTO user_metrics (
                    user_id, session_id, agent_id, team_id, total_messages,
                    session_duration_seconds, created_at, updated_at
                ) VALUES (
                    :user_id, :session_id, :agent_id, :team_id, :messages,
                    :duration, :timestamp, :timestamp
                ) ON CONFLICT (user_id, session_id)
                DO UPDATE SET
                    total_messages = user_metrics.total_messages + :messages,
                    updated_at = :timestamp
            """), {
                'user_id': data.get('user_id', 'anonymous'),
                'session_id': data.get('session_id'),
                'agent_id': data.get('agent_id'),
                'team_id': data.get('team_id'),
                'messages': data.get('message_count', 1),
                'duration': data.get('duration_seconds', 0),
                'timestamp': datetime.now()
            })
            
            db.commit()
            logger.debug(f"üë§ M√©trica de sess√£o salva diretamente no banco")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar sess√£o no banco: {e}")
            db.rollback()
        finally:
            db.close()

    # ===============================
    # PERSIST√äNCIA NO BANCO DE DADOS
    # ===============================
    
    def _write_to_db(self, query: str, params: Dict[str, Any]):
        """Escrever dados no banco de forma s√≠ncrona"""
        try:
            db = next(get_db())
            db.execute(text(query), params)
            db.commit()
            db.close()
        except Exception as e:
            logger.error(f"‚ùå Erro ao escrever no banco: {e}")
    
    async def _persist_user_metrics(self, data: Dict[str, Any]):
        """Persistir m√©tricas de usu√°rio"""
        query = """
        INSERT INTO user_metrics (user_id, session_id, agent_id, team_id, total_messages, session_duration_seconds)
        VALUES (:user_id, :session_id, :agent_id, :team_id, :total_messages, :session_duration_seconds)
        ON CONFLICT (user_id, session_id) 
        DO UPDATE SET 
            total_messages = user_metrics.total_messages + :total_messages,
            session_duration_seconds = :session_duration_seconds,
            updated_at = NOW()
        """
        self._write_to_db(query, data)
    
    async def _persist_token_usage(self, data: Dict[str, Any]):
        """Persistir consumo de tokens"""
        query = """
        INSERT INTO token_usage (agent_id, session_id, model_used, input_tokens, output_tokens, cost_estimate, operation_type)
        VALUES (:agent_id, :session_id, :model_used, :input_tokens, :output_tokens, :cost_estimate, :operation_type)
        """
        self._write_to_db(query, data)
    
    async def _persist_content_topics(self, data: Dict[str, Any]):
        """Persistir an√°lise de conte√∫do"""
        query = """
        INSERT INTO content_topics (session_id, agent_id, extracted_topics, message_content, topic_keywords, confidence_score)
        VALUES (:session_id, :agent_id, :extracted_topics, :message_content, :topic_keywords, :confidence_score)
        """
        self._write_to_db(query, data)


# ===============================
# INST√ÇNCIA GLOBAL
# ===============================

# Inst√¢ncia global do coletor
metrics_collector = MetricsCollector()

# Fun√ß√µes de conveni√™ncia para usar em outros m√≥dulos
async def init_metrics_system():
    """Inicializar sistema de m√©tricas"""
    success = await metrics_collector.initialize()
    if success:
        # N√£o iniciar workers aqui - ser√° feito no startup do FastAPI
        logger.info("‚úÖ Sistema de m√©tricas inicializado")
    return success

async def collect_agent_execution(agent_id: int, session_id: str, 
                                input_text: str, output_text: str,
                                execution_time_ms: int, model: str):
    """Coletar m√©tricas de execu√ß√£o de agente"""
    input_tokens = metrics_collector.estimate_tokens(input_text)
    output_tokens = metrics_collector.estimate_tokens(output_text)
    cost = metrics_collector.calculate_cost(model, input_tokens, output_tokens)
    
    await metrics_collector.collect_execution_metrics({
        'agent_id': agent_id,
        'session_id': session_id,
        'input_tokens': input_tokens,
        'output_tokens': output_tokens,
        'cost_estimate': cost,
        'execution_time_ms': execution_time_ms,
        'model': model,
        'operation_type': 'chat',
        'timestamp': datetime.now().isoformat()
    })

async def collect_chat_session(session_id: str, user_id: str, 
                              agent_id: Optional[int] = None,
                              team_id: Optional[int] = None,
                              message_count: int = 1):
    """Coletar m√©tricas de sess√£o de chat"""
    await metrics_collector.collect_session_metrics({
        'session_id': session_id,
        'user_id': user_id,
        'agent_id': agent_id,
        'team_id': team_id,
        'message_count': message_count,
        'timestamp': datetime.now().isoformat()
    })

async def collect_message_content(session_id: str, message_content: str,
                                agent_id: Optional[int] = None):
    """Coletar conte√∫do de mensagem para an√°lise"""
    await metrics_collector.collect_content_metrics({
        'session_id': session_id,
        'agent_id': agent_id,
        'message_content': message_content,
        'timestamp': datetime.now().isoformat()
    })

async def request_conversation_analysis(session_id: str, messages: List[Dict]):
    """Solicitar an√°lise inteligente de conversa"""
    conversation_data = {
        'session_id': session_id,
        'messages': messages,
        'user_id': messages[0].get('user_id', 'anonymous') if messages else 'anonymous',
        'agent_id': messages[-1].get('agent_id') if messages else None
    }
    
    await metrics_collector.request_conversation_classification(session_id, conversation_data)

def sync_request_conversation_analysis(session_id: str, messages: List[Dict]):
    """Vers√£o s√≠ncrona para an√°lise de conversa (para uso em threads)"""
    try:
        import asyncio
        conversation_data = {
            'session_id': session_id,
            'messages': messages,
            'user_id': messages[0].get('user_id', 'anonymous') if messages else 'anonymous',
            'agent_id': messages[-1].get('agent_id') if messages else None
        }
        
        # Executar de forma s√≠ncrona
        asyncio.run(metrics_collector.request_conversation_classification(session_id, conversation_data))
        
    except Exception as e:
        logger.error(f"‚ùå Erro na an√°lise s√≠ncrona: {e}")
        
        # Fallback direto no banco
        try:
            from database import SessionLocal
            from sqlalchemy import text
            
            db = SessionLocal()
            db.execute(text("""
                INSERT INTO user_feedback (
                    session_id, user_id, agent_id, rating, 
                    issue_category, feedback_comment, sentiment, auto_generated, created_at
                ) VALUES (
                    :session_id, :user_id, :agent_id, :rating,
                    :category, :comment, :sentiment, :auto_generated, NOW()
                )
            """), {
                'session_id': session_id,
                'user_id': conversation_data.get('user_id', 'sync_analysis'),
                'agent_id': conversation_data.get('agent_id'),
                'rating': 3,  # Rating neutro para an√°lises autom√°ticas
                'category': 'auto_analysis',
                'comment': f'An√°lise autom√°tica - {len(messages)} mensagens processadas',
                'sentiment': 'neutro',
                'auto_generated': True
            })
            db.commit()
            db.close()
            logger.info(f"‚úÖ An√°lise s√≠ncrona salva como fallback: {session_id}")
        except Exception as fallback_error:
            logger.error(f"‚ùå Erro no fallback s√≠ncrono: {fallback_error}")